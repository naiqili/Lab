{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files to ../../data\n",
      "extracting glove.840B.300d.txt\n",
      "extracting trees/train.txt\n",
      "extracting trees/dev.txt\n",
      "extracting trees/test.txt\n",
      "extracting stanfordSentimentTreebank/SOStr.txt\n",
      "total word count: 21701\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='None'\n",
    "import tensorflow as tf\n",
    "import tensorflow_fold as td\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "data_dir = '../../data'\n",
    "print('saving files to %s' % data_dir)\n",
    "\n",
    "def download_and_unzip(url_base, zip_name, *file_names):\n",
    "    zip_path = os.path.join(data_dir, zip_name)\n",
    "    url = url_base + zip_name\n",
    "    out_paths = []\n",
    "    if not os.path.exists(zip_path):\n",
    "        print('downloading %s to %s' % (url, zip_path))\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as f:\n",
    "        for file_name in file_names:\n",
    "            print('extracting %s' % file_name)\n",
    "            if not os.path.exists(os.path.join(zip_path, file_name)):\n",
    "                out_paths.append(f.extract(file_name, path=data_dir))      \n",
    "            else:\n",
    "                out_paths.append(file_name)\n",
    "    return out_paths\n",
    "\n",
    "full_glove_path, = download_and_unzip('http://nlp.stanford.edu/data/', 'glove.840B.300d.zip', 'glove.840B.300d.txt')\n",
    "filtered_glove_path = os.path.join(data_dir, 'filtered_glove.txt')\n",
    "\n",
    "train_path, dev_path, test_path = download_and_unzip(\n",
    "  'http://nlp.stanford.edu/sentiment/', 'trainDevTestTrees_PTB.zip', \n",
    "  'trees/train.txt', 'trees/dev.txt', 'trees/test.txt')\n",
    "\n",
    "def filter_glove():\n",
    "    vocab = set()\n",
    "    # Download the full set of unlabeled sentences separated by '|'.\n",
    "    sentence_path, = download_and_unzip(\n",
    "        'http://nlp.stanford.edu/~socherr/', 'stanfordSentimentTreebank.zip', \n",
    "        'stanfordSentimentTreebank/SOStr.txt')\n",
    "    with codecs.open(sentence_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Drop the trailing newline and strip backslashes. Split into words.\n",
    "            vocab.update(line.strip().replace('\\\\', '').split('|'))\n",
    "    print 'total word count:', len(vocab)\n",
    "    nread = 0\n",
    "    nwrote = 0\n",
    "    with codecs.open(full_glove_path, encoding='utf-8') as f:\n",
    "        with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                nread += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    nwrote += 1\n",
    "    print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "    print 'oov word count:', len(vocab)-nwrote\n",
    "\n",
    "filter_glove()\n",
    "\n",
    "def load_embeddings(embedding_path):\n",
    "    \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "    # '-RRB-' respectively in the parse-trees.\n",
    "    word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "    # Random embedding vector for unknown words.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "        -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    return np.stack(weight_vectors), word_idx\n",
    "\n",
    "weight_matrix, word_idx = load_embeddings(filtered_glove_path)\n",
    "\n",
    "print len(word_idx)\n",
    "print word_idx.items()[:20]\n",
    "\n",
    "def load_trees(filename):\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        # Drop the trailing newline and strip \\s.\n",
    "        trees = [line.strip().replace('\\\\', '') for line in f]\n",
    "        print('loaded %s trees from %s' % (len(trees), filename))\n",
    "    return trees\n",
    "\n",
    "class BinaryTreeLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "    def __init__(self, num_units, keep_prob=1.0):\n",
    "        super(BinaryTreeLSTMCell, self).__init__(num_units)\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            lhs, rhs = state\n",
    "            c0, h0 = lhs\n",
    "            c1, h1 = rhs\n",
    "            concat = tf.contrib.layers.linear(\n",
    "                tf.concat([inputs, h0, h1], 1), 5 * self._num_units)\n",
    "\n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f0, f1, o = tf.split(value=concat, num_or_size_splits=5, axis=1)\n",
    "\n",
    "        j = self._activation(j)\n",
    "        if not isinstance(self._keep_prob, float) or self._keep_prob < 1:\n",
    "            j = tf.nn.dropout(j, self._keep_prob)\n",
    "\n",
    "        new_c = (c0 * tf.sigmoid(f0 + self._forget_bias) + \\\n",
    "                 c1 * tf.sigmoid(f1 + self._forget_bias) + \\\n",
    "                 tf.sigmoid(i) * j)\n",
    "        new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return new_h, new_state\n",
    "\n",
    "keep_prob_ph = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "\n",
    "lstm_num_units = 300  # Tai et al. used 150, but our regularization strategy is more effective\n",
    "tree_lstm = td.ScopedLayer(\n",
    "      tf.contrib.rnn.DropoutWrapper(\n",
    "          BinaryTreeLSTMCell(lstm_num_units, keep_prob=keep_prob_ph),\n",
    "          input_keep_prob=keep_prob_ph, output_keep_prob=keep_prob_ph),\n",
    "      name_or_scope='tree_lstm')\n",
    "\n",
    "NUM_CLASSES = 5  # number of distinct sentiment labels\n",
    "output_layer = td.FC(NUM_CLASSES, activation=None, name='output_layer')\n",
    "\n",
    "word_embedding = td.Embedding(\n",
    "    *weight_matrix.shape, initializer=weight_matrix, name='word_embedding')\n",
    "\n",
    "def load_trees(filename):\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        # Drop the trailing newline and strip \\s.\n",
    "        trees = [line.strip().replace('\\\\', '') for line in f]\n",
    "        print('loaded %s trees from %s' % (len(trees), filename))\n",
    "        return trees\n",
    "\n",
    "train_trees = load_trees(train_path)\n",
    "dev_trees = load_trees(dev_path)\n",
    "test_trees = load_trees(test_path)\n",
    "\n",
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "\n",
    "def logits_and_state():\n",
    "    \"\"\"Creates a block that goes from tokens to (logits, state) tuples.\"\"\"\n",
    "    unknown_idx = len(word_idx)\n",
    "    lookup_word = lambda word: word_idx.get(word, unknown_idx)\n",
    "  \n",
    "    word2vec = (td.GetItem(0) >> td.InputTransform(lookup_word) >>\n",
    "                td.Scalar('int32') >> word_embedding)\n",
    "\n",
    "    pair2vec = (embed_subtree(), embed_subtree())\n",
    "\n",
    "    # Trees are binary, so the tree layer takes two states as its input_state.\n",
    "    zero_state = td.Zeros((tree_lstm.state_size,) * 2)\n",
    "    # Input is a word vector.\n",
    "    zero_inp = td.Zeros(word_embedding.output_type.shape[0])\n",
    "\n",
    "    word_case = td.AllOf(word2vec, zero_state)\n",
    "    pair_case = td.AllOf(zero_inp, pair2vec)\n",
    "\n",
    "    tree2vec = td.OneOf(len, [(1, word_case), (2, pair_case)])\n",
    "\n",
    "    return tree2vec >> tree_lstm >> (output_layer, td.Identity())\n",
    "\n",
    "\n",
    "def tf_node_loss(logits, labels):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "def tf_fine_grained_hits(logits, labels):\n",
    "    predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    return tf.cast(tf.equal(predictions, labels), tf.float64)\n",
    "\n",
    "\n",
    "def tf_binary_hits(logits, labels):\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    binary_predictions = (softmax[:, 3] + softmax[:, 4]) > (softmax[:, 0] + softmax[:, 1])\n",
    "    binary_labels = labels > 2\n",
    "    return tf.cast(tf.equal(binary_predictions, binary_labels), tf.float64)\n",
    "\n",
    "def add_metrics(is_root, is_neutral):\n",
    "    \"\"\"A block that adds metrics for loss and hits; output is the LSTM state.\"\"\"\n",
    "    c = td.Composition(\n",
    "        name='predict(is_root=%s, is_neutral=%s)' % (is_root, is_neutral))\n",
    "    with c.scope():\n",
    "        # destructure the input; (labels, (logits, state))\n",
    "        labels = c.input[0]\n",
    "        logits = td.GetItem(0).reads(c.input[1])\n",
    "        state = td.GetItem(1).reads(c.input[1])\n",
    "\n",
    "        # calculate loss\n",
    "        loss = td.Function(tf_node_loss)\n",
    "        td.Metric('all_loss').reads(loss.reads(logits, labels))\n",
    "        if is_root: td.Metric('root_loss').reads(loss)\n",
    "\n",
    "        # calculate fine-grained hits\n",
    "        hits = td.Function(tf_fine_grained_hits)\n",
    "        td.Metric('all_hits').reads(hits.reads(logits, labels))\n",
    "        if is_root: td.Metric('root_hits').reads(hits)\n",
    "\n",
    "        # calculate binary hits, if the label is not neutral\n",
    "        if not is_neutral:\n",
    "            binary_hits = td.Function(tf_binary_hits).reads(logits, labels)\n",
    "            td.Metric('all_binary_hits').reads(binary_hits)\n",
    "            if is_root: td.Metric('root_binary_hits').reads(binary_hits)\n",
    "\n",
    "        # output the state, which will be read by our by parent's LSTM cell\n",
    "        c.output.reads(state)\n",
    "    return c\n",
    "\n",
    "def tokenize(s):\n",
    "    label, phrase = s[1:-1].split(None, 1)\n",
    "    return label, sexpr.sexpr_tokenize(phrase)\n",
    "\n",
    "tokenize('(X Y Z)')\n",
    "\n",
    "def embed_tree(logits_and_state, is_root):\n",
    "    \"\"\"Creates a block that embeds trees; output is tree LSTM state.\"\"\"\n",
    "    return td.InputTransform(tokenize) >> td.OneOf(\n",
    "          key_fn=lambda pair: pair[0] == '2',  # label 2 means neutral\n",
    "          case_blocks=(add_metrics(is_root, is_neutral=False),\n",
    "                       add_metrics(is_root, is_neutral=True)),\n",
    "          pre_block=(td.Scalar('int32'), logits_and_state))\n",
    "\n",
    "model = embed_tree(logits_and_state(), is_root=True)\n",
    "\n",
    "embed_subtree.resolve_to(embed_tree(logits_and_state(), is_root=False))\n",
    "\n",
    "\n",
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)\n",
    "\n",
    "metrics = {k: tf.reduce_mean(v) for k, v in compiler.metric_tensors.items()}\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "KEEP_PROB = 0.75\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 20\n",
    "EMBEDDING_LEARNING_RATE_FACTOR = 0.1\n",
    "\n",
    "train_feed_dict = {keep_prob_ph: KEEP_PROB}\n",
    "loss = tf.reduce_sum(compiler.metric_tensors['all_loss'])\n",
    "opt = tf.train.AdagradOptimizer(LEARNING_RATE)\n",
    "\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "found = 0\n",
    "for i, (grad, var) in enumerate(grads_and_vars):\n",
    "    if var == word_embedding.weights:\n",
    "        found += 1\n",
    "        grad = tf.scalar_mul(EMBEDDING_LEARNING_RATE_FACTOR, grad)\n",
    "        grads_and_vars[i] = (grad, var)\n",
    "assert found == 1  # internal consistency check\n",
    "train = opt.apply_gradients(grads_and_vars)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train_step(batch):\n",
    "    train_feed_dict[compiler.loom_input_tensor] = batch\n",
    "    _, batch_loss = sess.run([train, loss], train_feed_dict)\n",
    "    return batch_loss\n",
    "\n",
    "def train_epoch(train_set):\n",
    "    return sum(train_step(batch) for batch in td.group_by_batches(train_set, BATCH_SIZE))\n",
    "\n",
    "\n",
    "train_set = compiler.build_loom_inputs(train_trees)\n",
    "dev_feed_dict = compiler.build_feed_dict(dev_trees)\n",
    "\n",
    "def dev_eval(epoch, train_loss):\n",
    "    dev_metrics = sess.run(metrics, dev_feed_dict)\n",
    "    dev_loss = dev_metrics['all_loss']\n",
    "    dev_accuracy = ['%s: %.2f' % (k, v * 100) for k, v in \\\n",
    "                  sorted(dev_metrics.items()) if k.endswith('hits')]\n",
    "    print('epoch:%4d, train_loss: %.3e, dev_loss_avg: %.3e, dev_accuracy:\\n  [%s]' \\\n",
    "        % (epoch, train_loss, dev_loss, ' '.join(dev_accuracy)))\n",
    "    return dev_metrics['root_hits']\n",
    "\n",
    "best_accuracy = 0.0\n",
    "save_path = os.path.join(data_dir, 'sentiment_model')\n",
    "for epoch, shuffled in enumerate(td.epochs(train_set, EPOCHS), 1):\n",
    "    train_loss = train_epoch(shuffled)\n",
    "    accuracy = dev_eval(epoch, train_loss)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        checkpoint_path = saver.save(sess, save_path, global_step=epoch)\n",
    "        print('model saved in file: %s' % checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
