{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_fold as td\n",
    "import cPickle\n",
    "import config\n",
    "import numpy as np\n",
    "from nltk.tokenize import sexpr\n",
    "from utils import *\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "test_str = '(2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))'\n",
    "print(tokenize(test_str))\n",
    "print('\\n'*2)\n",
    "print_senti_tree(test_str)\n",
    "\n",
    "def block_info(block):\n",
    "    print(\"%s: %s -> %s\" % (block, block.input_type, block.output_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Map element_block=<td.Scalar dtype='float32'>>: None -> SequenceType(TensorType((), 'float32'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(1.0, dtype=float32),\n",
       " array(2.0, dtype=float32),\n",
       " array(3.0, dtype=float32)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = td.Map(td.Scalar())\n",
    "block_info(temp)\n",
    "\n",
    "temp.eval([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Pipe>: None -> TupleType(SequenceType(TensorType((), 'float32')), SequenceType(TensorType((), 'float32')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array(1.0, dtype=float32),\n",
       "  array(2.0, dtype=float32),\n",
       "  array(3.0, dtype=float32)],\n",
       " [array(4.0, dtype=float32),\n",
       "  array(5.0, dtype=float32),\n",
       "  array(6.0, dtype=float32)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = (td.Map(td.Scalar()), td.Map(td.Scalar())) >> td.Identity()\n",
    "block_info(temp)\n",
    "\n",
    "temp.eval(([1,2,3],[4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Pipe>: PyObjectType() -> TensorType((6,), 'float32')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = (td.Vector(3), td.Vector(3))  >> td.Concat()\n",
    "block_info(temp)\n",
    "\n",
    "temp.eval(([1,2,3],[4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = td.Composition()\n",
    "with c.scope():\n",
    "    x = c.input[0]\n",
    "    c.output.reads(x)\n",
    "                  \n",
    "((td.Scalar(), td.Scalar()) >> c).eval((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = td.Composition()\n",
    "with c.scope():\n",
    "    x = c.input\n",
    "    c.output.reads(x)\n",
    "                  \n",
    "(td.Scalar() >> c).eval(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tuple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.,  24.,  39.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = td.Composition()\n",
    "with c.scope():\n",
    "    x = td.Vector(3).reads(c.input)\n",
    "    x_squared = td.Function(tf.multiply).reads(x, x)\n",
    "    ten = td.FromTensor(10 * np.ones(3, dtype='float32'))\n",
    "    ten_x = td.Function(tf.multiply).reads(ten, x)\n",
    "    c.output.reads(td.Function(tf.add).reads(x_squared, ten_x))\n",
    "                   \n",
    "c.eval([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(202.0, dtype=float32), array(101.0, dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = td.Composition()\n",
    "with pair.scope():\n",
    "    a = td.Function(lambda x: x + 100).reads(pair.input[0])\n",
    "    b = td.Function(lambda x: x + 200).reads(pair.input[1])\n",
    "    pair.output.reads(b, a)\n",
    "    \n",
    "((td.Scalar(), td.Scalar()) >> pair).eval((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(2.0, dtype=float32), array(1.0, dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = td.AllOf(td.Map(td.Scalar()) >> td.GetItem(1), td.Map(td.Scalar()) >> td.GetItem(0))\n",
    "pair.eval((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(11.0, dtype=float32), array(21.0, dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = td.AllOf(td.Map(td.Scalar()) >> td.GetItem(0) >> td.Function(lambda x: x + 10),\n",
    "                td.Map(td.Scalar()) >> td.GetItem(0) >> td.Function(lambda x: x + 20))\n",
    "    \n",
    "pair.eval((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = td.AllOf(td.GetItem(1), td.GetItem(0)) >> td.Concat()\n",
    "((td.Scalar(), td.Scalar()) >> pair).eval((1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.]\n",
      "[ 3.  1.  2.]\n",
      "[ 1.  2.  3.]\n",
      "[ 2.  3.  4.  5.]\n",
      "[ 1.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "def get_c1():\n",
    "    return (td.Scalar(), td.Scalar()) >> td.Concat()\n",
    "\n",
    "c1 = get_c1()\n",
    "print c1.eval((1,2))\n",
    "\n",
    "c2 = (td.Scalar(), c1) >> td.Concat()\n",
    "print c2.eval((3,(1,2)))\n",
    "\n",
    "c3 = (td.Scalar(), td.Scalar(), td.Scalar()) >> td.Concat()\n",
    "print c3.eval((1,2,3))\n",
    "\n",
    "c4 = (get_c1(), get_c1()) >> td.Concat()\n",
    "print c4.eval(((2,3),(4,5)))\n",
    "\n",
    "c5 = (td.Scalar(), get_c1(), get_c1()) >> td.Concat()\n",
    "print c5.eval((1,(2,3),(4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Vector dtype='float32' size=3>: PyObjectType() -> TensorType((3,), 'float32')\n",
      "[ 7.  8.  9.]\n",
      "<td.Pipe>: PyObjectType() -> TupleType(TensorType((), 'float32'), TensorType((3,), 'float32'))\n",
      "(array(1.0, dtype=float32), array([ 7.,  8.,  9.], dtype=float32))\n",
      "<td.Pipe>: PyObjectType() -> TensorType((4,), 'float32')\n",
      "[ 1.  7.  8.  9.]\n",
      "[ 1.  2.  3.  7.  8.  9.]\n"
     ]
    }
   ],
   "source": [
    "v_block = td.Vector(3)\n",
    "block_info(v_block)\n",
    "print v_block.eval([7,8,9])\n",
    "\n",
    "temp = (td.Scalar(), v_block) >> td.Identity()\n",
    "block_info(temp)\n",
    "print temp.eval((1, [7,8,9]))\n",
    "\n",
    "temp = temp >> td.Concat()\n",
    "block_info(temp)\n",
    "print temp.eval((1, [7,8,9]))\n",
    "\n",
    "print ((td.Vector(3), td.Vector(3)) >> td.Concat()).eval(([1,2,3], [7,8,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label a s-exp with node ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.0, dtype=float32), '2', '(3 (3 Effective) (2 but))')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_case = td.Composition()\n",
    "with word_case.scope():\n",
    "    node_id = word_case.input[0]\n",
    "    label = (td.InputTransform(tokenize) >> td.GetItem(0)).reads(word_case.input[1])\n",
    "    s = (td.InputTransform(tokenize) >> td.GetItem(1)).reads(word_case.input[1])\n",
    "    left_s = td.GetItem(0).reads(s)\n",
    "    word_case.output.reads(node_id, label, left_s)\n",
    "((td.Scalar(), td.Identity()) >> word_case).eval((1, test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(3 (3 Effective) (2 but))', '(1 (1 too-tepid) (2 biopic))']\n",
      "(array(1.0, dtype=float32), '(2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))')\n"
     ]
    }
   ],
   "source": [
    "temp = td.GetItem(1) >> td.InputTransform(tokenize) >> td.GetItem(1) >> td.OneOf(len, \n",
    "                                                                                 [(1, td.Identity()), (2, td.Identity())])\n",
    "print ((td.Scalar(), td.Identity()) >> temp).eval((1, test_str))\n",
    "\n",
    "temp = td.OneOf(td.GetItem(1) >> td.InputTransform(tokenize) >> td.GetItem(1) >> td.InputTransform(len), \n",
    "                [(1, td.Identity()), (2, td.Identity())])\n",
    "print ((td.Scalar(), td.Identity()) >> temp).eval((1, test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.OneOf>: PyObjectType() -> TupleType(SequenceType(TensorType((), 'float32')), PyObjectType())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array(1.0, dtype=float32)],\n",
       " ('2', ['(3 (3 Effective) (2 but))', '(1 (1 too-tepid) (2 biopic))']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = td.OneOf(lambda x: len(tokenize(x[1])[1]), \n",
    "                         {1: (td.InputTransform(lambda x: [x]) >> td.Map(td.Scalar()), td.Identity()) >> td.Identity(), \n",
    "                          2: (td.InputTransform(lambda x: [x]) >> td.Map(td.Scalar()), td.InputTransform(tokenize))})\n",
    "\n",
    "block_info(temp)\n",
    "(temp).eval((1, test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.OneOf>: None -> None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(1.0, dtype=float32),\n",
       " '(2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_tree():\n",
    "    tree_case = td.OneOf(td.GetItem(1) >> td.InputTransform(lambda x: len(tokenize(x)[1])), \n",
    "                         {1: (td.Scalar(), td.Identity()), 2: (td.Scalar(), td.Identity())})\n",
    "    return  tree_case\n",
    "\n",
    "model = embed_tree()\n",
    "block_info(model)\n",
    "\n",
    "(model).eval((1, test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Composition>: None -> None\n",
      "<td.GetItem key=0>: None -> None\n",
      "<td.Pipe>: None -> None\n",
      "<td.OneOf>: None -> None\n",
      "<td.GetItem key=0>: None -> None\n",
      "<td.Pipe>: None -> None\n",
      "<td.OneOf>: None -> None\n",
      "<td.Pipe>: None -> None\n",
      "input type: PyObjectType()\n",
      "output type: TensorType((), 'float32')\n",
      "[ 4.  5.  2.  6.  7.  3.  1.]\n"
     ]
    }
   ],
   "source": [
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "\n",
    "def add_metrics():\n",
    "    c = td.Composition()\n",
    "    with c.scope():\n",
    "        node_id = c.input\n",
    "        \n",
    "        td.Metric('node_ids').reads(node_id)\n",
    "        \n",
    "        c.output.reads(node_id)\n",
    "    return c\n",
    "\n",
    "block_info(add_metrics())\n",
    "\n",
    "def embed_tree():\n",
    "    \"\"\"Input: node_id, s-exp\n",
    "       Output: node_ids\"\"\"\n",
    "    \n",
    "    word_case = td.GetItem(0)\n",
    "    block_info(word_case)\n",
    "    \n",
    "    pair_case = td.AllOf(td.Identity(),\n",
    "                         (td.Function(lambda x: 2*x), \n",
    "                          td.InputTransform(tokenize) >> td.InputTransform(lambda x: x[1][0])) \n",
    "                             >> embed_subtree(),\n",
    "                         (td.Function(lambda x: 2*x+1), \n",
    "                          td.InputTransform(tokenize) >> td.InputTransform(lambda x: x[1][1])) \n",
    "                             >> embed_subtree()\n",
    "                         ) >> td.GetItem(0) >> td.GetItem(0)\n",
    "    block_info(pair_case)\n",
    "    \n",
    "    tree_case = td.OneOf(td.GetItem(1) >> td.InputTransform(lambda x: len(tokenize(x)[1])), \n",
    "                         {1: word_case, 2: pair_case})\n",
    "    block_info(tree_case)\n",
    "    \n",
    "    return tree_case >> add_metrics()\n",
    "\n",
    "\n",
    "model = (td.Scalar(), td.Identity()) >> embed_tree()\n",
    "\n",
    "embed_subtree.resolve_to(embed_tree())\n",
    "block_info(model)                 \n",
    "\n",
    "#model.eval((1, test_str))\n",
    "\n",
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)\n",
    "\n",
    "metrics = compiler.metric_tensors['node_ids']\n",
    "\n",
    "res = sess.run(metrics,\n",
    "         compiler.build_feed_dict([(1, test_str)]))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td.Composition>: None -> None\n",
      "<td.GetItem key=0>: None -> None\n",
      "<td.Composition>: None -> None\n",
      "<td.OneOf>: None -> None\n",
      "<td.GetItem key=0>: None -> None\n",
      "<td.Composition>: None -> None\n",
      "<td.OneOf>: None -> None\n",
      "<td.Pipe>: None -> None\n",
      "input type: PyObjectType()\n",
      "output type: TensorType((), 'float32')\n",
      "[ 4.  5.  2.  6.  7.  3.  1.]\n"
     ]
    }
   ],
   "source": [
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "\n",
    "def add_metrics():\n",
    "    c = td.Composition()\n",
    "    with c.scope():\n",
    "        node_id = c.input\n",
    "        \n",
    "        td.Metric('node_ids').reads(node_id)\n",
    "        \n",
    "        c.output.reads(node_id)\n",
    "    return c\n",
    "\n",
    "block_info(add_metrics())\n",
    "\n",
    "def embed_tree():\n",
    "    \"\"\"Input: node_id, s-exp\n",
    "       Output: node_ids\"\"\"\n",
    "    \n",
    "    word_case = td.GetItem(0)\n",
    "    block_info(word_case)\n",
    "    \n",
    "    pair_case = td.Composition()\n",
    "    with pair_case.scope():\n",
    "        node_id = pair_case.input[0]\n",
    "        left_node_id = td.Function(lambda x: 2*x).reads(node_id)\n",
    "        right_node_id = td.Function(lambda x: 2*x+1).reads(node_id)\n",
    "        tree_s = pair_case.input[1]\n",
    "        tok_res = td.InputTransform(tokenize).reads(tree_s)\n",
    "        left_tree_s = (td.GetItem(1) >> td.GetItem(0)).reads(tok_res)\n",
    "        right_tree_s = (td.GetItem(1) >> td.GetItem(1)).reads(tok_res)\n",
    "        left_ret = embed_subtree().reads(left_node_id, left_tree_s)\n",
    "        right_ret = embed_subtree().reads(right_node_id, right_tree_s)\n",
    "        pair_case.output.reads(node_id, left_ret, right_ret)\n",
    "        \n",
    "    block_info(pair_case)\n",
    "    \n",
    "    tree_case = td.OneOf(td.GetItem(1) >> td.InputTransform(lambda x: len(tokenize(x)[1])), \n",
    "                         {1: word_case, 2: pair_case >> td.GetItem(0)})\n",
    "    block_info(tree_case)\n",
    "    \n",
    "    return tree_case >> add_metrics()\n",
    "\n",
    "\n",
    "model = (td.Scalar(), td.Identity()) >> embed_tree()\n",
    "\n",
    "embed_subtree.resolve_to(embed_tree())\n",
    "block_info(model)                 \n",
    "\n",
    "#model.eval((1, test_str))\n",
    "\n",
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)\n",
    "\n",
    "metrics = compiler.metric_tensors['node_ids']\n",
    "\n",
    "res = sess.run(metrics,\n",
    "         compiler.build_feed_dict([(1, test_str)]))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')\n",
    "\n",
    "class BinaryTreeLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "    def __init__(self, num_units, keep_prob=1.0):\n",
    "        super(BinaryTreeLSTMCell, self).__init__(num_units)\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            lhs, rhs = state\n",
    "            c0, h0 = lhs\n",
    "            c1, h1 = rhs\n",
    "            concat = tf.contrib.layers.linear(\n",
    "                tf.concat([inputs, h0, h1], 1), 5 * self._num_units)\n",
    "\n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f0, f1, o = tf.split(value=concat, num_or_size_splits=5, axis=1)\n",
    "\n",
    "        j = self._activation(j)\n",
    "        if not isinstance(self._keep_prob, float) or self._keep_prob < 1:\n",
    "            j = tf.nn.dropout(j, self._keep_prob)\n",
    "\n",
    "        new_c = (c0 * tf.sigmoid(f0 + self._forget_bias) + \\\n",
    "                 c1 * tf.sigmoid(f1 + self._forget_bias) + \\\n",
    "                 tf.sigmoid(i) * j)\n",
    "        new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return new_h, new_state\n",
    "    \n",
    "\n",
    "NUM_CLASSES = 5  # number of distinct sentiment labels\n",
    "embed_size = 7\n",
    "state_size = 5\n",
    "tree_lstm = td.ScopedLayer(\n",
    "      tf.contrib.rnn.DropoutWrapper(\n",
    "          BinaryTreeLSTMCell(state_size)),\n",
    "      name_or_scope='tree_lstm')\n",
    "\n",
    "output_layer = td.FC(NUM_CLASSES, activation=None, name='output_layer')\n",
    "\n",
    "def tf_node_loss(logits, labels):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "def tf_fine_grained_hits(logits, labels):\n",
    "    predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    return tf.cast(tf.equal(predictions, labels), tf.float64)\n",
    "\n",
    "def tf_binary_hits(logits, labels):\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    binary_predictions = (softmax[:, 3] + softmax[:, 4]) > (softmax[:, 0] + softmax[:, 1])\n",
    "    binary_labels = labels > 2\n",
    "    return tf.cast(tf.equal(binary_predictions, binary_labels), tf.float64)\n",
    "\n",
    "def add_metrics():\n",
    "    \"\"\"A block that adds metrics for loss and hits; output is the LSTM state.\"\"\"\n",
    "    c = td.Composition()\n",
    "    with c.scope():\n",
    "        # destructure the input; (id, (logits, state))\n",
    "        node_id = c.input[0]\n",
    "        label = c.input[1]\n",
    "        \n",
    "        logits = td.GetItem(0).reads(c.input[2])\n",
    "        state = td.GetItem(1).reads(c.input[2])\n",
    "        \n",
    "        td.Metric('node_ids').reads(node_id)\n",
    "        td.Metric('logits').reads(logits)\n",
    "        td.Metric('labels').reads(label)\n",
    "\n",
    "        # output the state, which will be read by our by parent's LSTM cell\n",
    "        c.output.reads(state)\n",
    "    return c\n",
    "\n",
    "def logits_and_state():\n",
    "    \"\"\"Creates a block that goes from (node_id, tree) to (node_id, label, (logits, state)) tuples.\"\"\"\n",
    "\n",
    "    pair2vec = td.Composition()\n",
    "    with pair2vec.scope():\n",
    "        node_id = pair2vec.input[0]\n",
    "        left_node_id = td.Function(lambda x: 2*x).reads(node_id)\n",
    "        right_node_id = td.Function(lambda x: 2*x+1).reads(node_id)\n",
    "        tree_s = pair2vec.input[1]\n",
    "        tok_res = td.InputTransform(tokenize).reads(tree_s)\n",
    "        left_tree_s = (td.GetItem(1) >> td.GetItem(0)).reads(tok_res)\n",
    "        right_tree_s = (td.GetItem(1) >> td.GetItem(1)).reads(tok_res)\n",
    "        left_ret = embed_subtree().reads(left_node_id, left_tree_s)\n",
    "        right_ret = embed_subtree().reads(right_node_id, right_tree_s)\n",
    "        pair2vec.output.reads(left_ret, right_ret)\n",
    "    block_info(pair2vec)\n",
    "\n",
    "    \n",
    "    word_case = td.AllOf(td.FromTensor(np.random.rand(embed_size).astype(np.float32)), td.Zeros((tree_lstm.state_size,) * 2))\n",
    "    block_info(word_case)\n",
    "    \n",
    "    pair_case = td.AllOf(td.Zeros(embed_size), pair2vec)\n",
    "    block_info(pair_case)\n",
    "    \n",
    "    tree2vec = td.OneOf(td.GetItem(1) >> td.InputTransform(lambda x: len(tokenize(x)[1])), [(1, word_case), (2, pair_case)])\n",
    "    block_info(tree2vec)\n",
    "    \n",
    "    res = td.AllOf(td.GetItem(0), # node_id\n",
    "                   td.GetItem(1) >> td.InputTransform(lambda x: tokenize(x)[0]) >> td.Scalar('int32'),\n",
    "                   tree2vec >> tree_lstm >> (output_layer, td.Identity())\n",
    "                  )\n",
    "    return res\n",
    "\n",
    "def embed_tree(logits_and_state, is_root):\n",
    "    \"\"\"Creates a block that embeds (node_id, trees); output is tree LSTM state.\"\"\"\n",
    "    return logits_and_state >> add_metrics()\n",
    "\n",
    "model = (td.Scalar('int32'), td.Identity()) >> embed_tree(logits_and_state(), is_root=True)\n",
    "\n",
    "embed_subtree.resolve_to(embed_tree(logits_and_state(), is_root=False))\n",
    "block_info(model)                 \n",
    "\n",
    "\n",
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)\n",
    "\n",
    "metrics = compiler.metric_tensors['node_ids']\n",
    "logits = compiler.metric_tensors['logits']\n",
    "labels = compiler.metric_tensors['labels']\n",
    "\n",
    "print(metrics)\n",
    "print(logits)\n",
    "print(labels)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "res = sess.run([metrics, logits, labels],\n",
    "         compiler.build_feed_dict([(1, test_str)]))\n",
    "\n",
    "print(res)\n",
    "print('\\n'*2)\n",
    "print(res[0])\n",
    "print(type(res[0]))\n",
    "print(type(res[2]))\n",
    "print_senti_tree(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
